# -*- coding: utf-8 -*-
"""Capstone_project_606.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o61UZttkTMoVPLnDMGmLQ7lNJqfVYwcm
"""

#importing libraries

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from PIL import Image
from wordcloud import WordCloud, STOPWORDS
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from string import punctuation
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
import pickle
import streamlit as st

#loading data

df = pd.read_csv('/content/NetFlix.csv')

df

#Total columns

print(list(df.columns))

#Shape of the dataset
df.shape

# Description
df.describe()

# INformation about data

df.info()

# Checking duplicates
df.duplicated().sum()

"""**Exploratory Data Analysis**"""

#Count of movies and Tv shows

sns.countplot(x = 'type' ,data =df)
plt.title('Movies Vs Tv Shows')
plt.figure(figsize=(20,8))

# distribution of Movies and Tv shows

sns.set_palette('gist_ncar')
df.type.value_counts().plot(kind='pie',autopct='%1.0f%%',explode=(0.05,0.05))
plt.title('Distribution of type',fontsize=25,fontweight='bold')

# Checking the Countries that streaming movies 

plt.figure(figsize=(10,7))
sns.countplot(y=df['country'], order=df['country'].value_counts().index[0:20])
ax=plt.xticks(rotation = 0)

# Top 10 directors 

plt.figure(figsize=(20,10))
graph=sns.countplot(y='director',data=df,order=df.director.value_counts().head(10).index)
graph.set_title("top 10 Directors of Netflix",fontsize=15,fontweight='bold')

# Adding 2 more columns
 
df['date_added']=pd.to_datetime(df['date_added'])
df['year_added']=df['date_added'].dt.year
df['month_added']=df['date_added'].dt.month

df.head(5)

# Greater number of movies per year

df['year_added'].value_counts().reset_index().rename(columns={'index':'year','year_added':'movie_count'})

#percentage of movies per year

graph=px.pie(df,names='year_added', height=500,width=900, hole=0.3, title='Netflix year distribution')
graph.show()

# percentage of movies per month

graph=px.pie(df,names='month_added', height=500,width=900, hole=0.3, title='Netflix month distribution')
graph.show()

# displaying ratings

plt.figure(figsize=(10,7))
ax=sns.countplot(y=df['rating'], order=df['rating'].value_counts().index[0:])

#Display rating in percentage

graph=px.pie(df,names='rating', height=500,width=900, hole=0.3, title='Netflix rating distribution')
graph.show()

!pip install wordcloud

# checking the most common and popular word that used in title

df_wc=df['title']
text=" ".join(i for i in df_wc)

stopwords=set(STOPWORDS)

wordcloud=WordCloud(stopwords=stopwords, background_color='white').generate(text)

plt.imshow(wordcloud,interpolation='nearest')
plt.axis('off')
plt.show()

"""**Preprocessing**

**Stemming**
"""

stemmer = SnowballStemmer("english")

def Apply_stemming(text):
  text = [stemmer.stem(word) for word in text.split()]
  return " ".join(text)

#Applying stemming for better understanding of description

df['description'] = df['description'].apply(Apply_stemming)
df.head()

"""** Count Vectorization**"""

# Count vectorization to convert text into numerical data

cv=CountVectorizer(max_features=8000,stop_words='english')

cv

#merging important variables for easy process

df['tags']=df['description']+df['genres']

tags = df['tags']

tags

vector=cv.fit_transform(df['tags'].values.astype('U')).toarray()

vector.shape

"""**Cosine Similarity**"""



similarity = cosine_similarity(vector)

new_df = df.drop(columns=['description', 'genres'])

new_df

#function for recommending movies

def recommend(movie): 
  movie_index = df[df['title']==movie].index[0]
  distance = similarity[movie_index]
  movie_list = sorted(list(enumerate(distance)),reverse=True, key=lambda x:x[1])[1:6]

  for i in movie_list:
    print(df.iloc[i[0]].title)

#checking if that function works

recommend("3 Heroines")

"""**Sentiment Analysis**"""

#scaping data from google

movie_reviews = []
url = 'https://www.google.com/search?q=movie+reviews+avengers+endgame'

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
for review in soup.find_all('div', {'class': 'b1hJbf'}):
    movie_reviews.append(review.get_text())

#processing of removing punctuations and stopwords

nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))
punctuations = set(punctuation)

def preprocess_text(text):
    text = text.lower()

 # Tokenization 

    tokens = nltk.word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words and word not in punctuations]
    text = preprocessed_text(' '.join(tokens))
    return preprocessed_text

#giving sentiment to labels

labels =['positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive']

df = pd.DataFrame({'review':text, 'sentiment': labels})

#Splitting training and testing data

X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)

#Vectorization

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

"""**Naive Bayes **"""

nb = MultinomialNB()
nb.fit(X_train_vec, y_train)

y_pred = nb.predict(X_test_vec)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

pipeline = Pipeline([
    ('vectorizer', CountVectorizer(lowercase=True, stop_words='english', strip_accents='unicode')),
    ('classifier', MultinomialNB())
])

scores = cross_val_score(pipeline, df['review'], df['sentiment'], cv=3)

print(scores)

"""**K Nearest Neighbors**

"""

knn = KNeighborsClassifier(n_neighbors=5)

X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)

knn.fit(X_train_vec, y_train)

# Predict the sentiment of the test data
y_pred = knn.predict(X_test_vec)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

"""deployment"""

# Serializing

pickle.dump(new_df,open('df.pkl','wb'))
pickle.dump(similarity,open('similarity.pkl','wb'))

new_df['title'].values

new_df.to_dict()

pickle.dump(new_df.to_dict(), open('df_dict.pkl','wb'))

# Installing Streamlit

!pip install -q streamlit

#Funtion that are passing to Streamlit to recommend movies

for col in df.columns:
    print(col)
for col in new_df.columns:
    print(col)
print('D')
def recommend(movie): 
  print(df)
  movie_index = new_df[new_df['title']==movie].index[0]
  distance = similarity[movie_index]
  movie_list = sorted(list(enumerate(distance)),reverse=True, key=lambda x:x[1])[1:6]
  recommended_movies = []
  for i in movie_list:
    recommended_movies.append(new_df.iloc[i[0]].title)
  return recommended_movies

df_dict = pickle.load(open('df_dict.pkl','rb'))
tags = pd.DataFrame(df_dict)

similarity = pickle.load(open('similarity.pkl','rb'))

st.title('Netflix Recommendation system')

option= st.selectbox('select your movie:', tags['title'].values)

if st.button('Similar movies'):
  recommendations = recommend(option)
  for i in recommendations:
    st.write(i)

!npm install localtunnel

!streamlit run /content/file_py.ipynb &>/content/logs.txt &

# Access using this local host link

!npx localtunnel --port 8501